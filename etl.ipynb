{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb8f3732ff94b57a5d59db41e105594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1612115595800_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-35-59.us-east-2.compute.internal:20888/proxy/application_1612115595800_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-40-30.us-east-2.compute.internal:8042/node/containerlogs/container_1612115595800_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, dayofweek, date_format, monotonically_increasing_id\n",
    "from pyspark.sql.types import TimestampType, DateType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65eab3b6068490a817c2b1f0f83e3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "        Reads song_data .json files from S3, processes them, extracts artists and songs tables and writes them back to S3 as parquet files.\n",
    "       \n",
    "        Parameters:\n",
    "            spark       : Spark Session\n",
    "            input_data  : S3 bucket path for input files\n",
    "            output_data : S3 bucket path for output files\n",
    "    \"\"\"\n",
    "    # takes input filepath\n",
    "    song_data = input_data + 'song_data/*/*/*/*.json'\n",
    "\n",
    "    # reads song data files\n",
    "    df = spark.read.json(song_data)\n",
    "\n",
    "    # extracts columns for songs while droping duplicates\n",
    "    songs_table = df.select(\"song_id\",\"title\",\"artist_id\",\"year\",\"duration\").drop_duplicates()\n",
    "\n",
    "    # overwrites parquet files in partitioned file format\n",
    "    songs_table.write.partitionBy(\"year\", \"artist_id\").parquet(\"{}songs/songs_table.parquet\".format(output_data),mode=\"overwrite\")\n",
    "\n",
    "    # extracts columns for artists while droping duplicates\n",
    "    artists_table = df.select(\"artist_id\",\"artist_name\",\"artist_location\",\"artist_latitude\",\"artist_longitude\").drop_duplicates()\n",
    "    \n",
    "    # overwrites parquet files in partitioned file format\n",
    "    artists_table.write.parquet(\"{}artists/artists_table.parquet\".format(output_data),mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78db7f29a114fd09a0ff2300c490abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def process_log_data(spark, input_data, output_data):\n",
    "    \n",
    "    \"\"\"\n",
    "        Reads log_data .json files from S3, processes them, extracts users, timestamp and songplays tables and writes them back to S3 as parquet files.\n",
    "        \n",
    "        Parameters:\n",
    "            spark       : Spark Session\n",
    "            input_data  : S3 bucket path for input files\n",
    "            output_data : S3 bucket path for output files\n",
    "    \"\"\"\n",
    "    # takes input filepath\n",
    "    log_data = input_data + 'log_data/*/*/*.json'\n",
    "    \n",
    "    # reads log data files\n",
    "    log_df = spark.read.json(log_data)\n",
    "\n",
    "    # applies filter by choosing only NextSong values for page field\n",
    "    log_df = log_df.filter(log_df.page == \"NextSong\").cache()\n",
    "\n",
    "    # extracts columns for users while droping duplicates\n",
    "    users_table = log_df.select(\"userId\",\"firstName\",\"lastName\",\"gender\",\"level\").drop_duplicates()\n",
    "\n",
    "    # overwrites parquet files in partitioned file format\n",
    "    users_table.write.parquet(\"{}users/users_table.parquet\".format(output_data),mode=\"overwrite\")\n",
    "    \n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = udf(lambda x: datetime.fromtimestamp(x / 1000), TimestampType())\n",
    "    log_df = log_df.withColumn(\"start_time\", get_timestamp(col(\"ts\")))\n",
    "\n",
    "    # extracts columns for users while droping duplicates\n",
    "    time_table = log_df.withColumn(\"hour\",hour(\"start_time\"))\\\n",
    "                    .withColumn(\"day\",dayofmonth(\"start_time\"))\\\n",
    "                    .withColumn(\"week\",weekofyear(\"start_time\"))\\\n",
    "                    .withColumn(\"month\",month(\"start_time\"))\\\n",
    "                    .withColumn(\"year\",year(\"start_time\"))\\\n",
    "                    .withColumn(\"weekday\",dayofweek(\"start_time\"))\\\n",
    "                    .select(\"ts\",\"start_time\",\"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\").drop_duplicates()\n",
    "    \n",
    "\n",
    "    # overwrites parquet files in partitioned file format\n",
    "    time_table.write.partitionBy(\"year\", \"month\").parquet(\"{}time/time_table.parquet\".format(output_data),mode=\"overwrite\")\n",
    "\n",
    "     # reads songs files to create songs_df in order to join with log data to extract songplays\n",
    "    songs_df = spark.read\\\n",
    "                .format(\"parquet\")\\\n",
    "                .option(\"basePath\", os.path.join(output_data, \"songs/\"))\\\n",
    "                .load(os.path.join(output_data, \"songs/*/*/\"))\n",
    "    \n",
    "    \n",
    "    # extracts columns for songplays by joining songs_df and log_df\n",
    "    songplays_table = log_df.join(songs_df, log_df.song == songs_df.title, how='inner')\\\n",
    "                        .select(monotonically_increasing_id().alias(\"songplay_id\"),col(\"start_time\"),col(\"userId\").alias(\"user_id\"),\"level\",\"song_id\",\"artist_id\", col(\"sessionId\").alias(\"session_id\"), \"location\", col(\"userAgent\").alias(\"user_agent\"))\n",
    "\n",
    "    # extracts columns for songplays by joining songplays_table and time_table   \n",
    "    songplays_table = songplays_table.join(time_table, songplays_table.start_time == time_table.start_time, how=\"inner\")\\\n",
    "                        .select(\"songplay_id\", songplays_table.start_time, \"user_id\", \"level\", \"song_id\", \"artist_id\", \"session_id\", \"location\", \"user_agent\", \"year\", \"month\")\n",
    "    \n",
    "    \n",
    "    # overwrites parquet files in partitioned file format\n",
    "    songplays_table.write.partitionBy(\"year\", \"month\") \\\n",
    "        .parquet(\"{}songplays/songplays_table.parquet\".format(output_data),mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad829e2ff6744368f30ce91c6f3104b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    # input and output paths\n",
    "    input_data = \"s3a://udacity-dend/\"\n",
    "    output_data = \"s3://celebis/sparkify-output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5110a3d4e2e2474a9728b1be74d944bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    # processing song data\n",
    "process_song_data(spark, input_data, output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1f753486fe4f24962e8681f2bab633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    # processing log data\n",
    "process_log_data(spark, input_data, output_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
